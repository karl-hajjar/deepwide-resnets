{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319faced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15e6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437774ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlhajjar/Documents/projects/deep-wide-resnets/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from copy import deepcopy\n",
    "\n",
    "import scipy\n",
    "from scipy import special as s\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9021c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd+'/')\n",
    "ROOT = os.path.dirname(NOTEBOOK_DIR)\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08500acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot import *\n",
    "from utils.tools import *\n",
    "from utils.nn import TARGET_FUNCS_DICT, LOSS_DICT\n",
    "from networks.muP_resnet import MuPResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06288d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fedbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 64 #30\n",
    "WIDTH = 256 #512 #512 # 512 #40 # 256\n",
    "#D_MODEL = 128\n",
    "N_RES = 500 #1000\n",
    "BIAS = False\n",
    "ALPHA = 1.0\n",
    "SCALE = 1.0\n",
    "ACTIVATION = 'relu'\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "#N_TRIALS = 10 #10\n",
    "BASE_LR = 1.0e-2\n",
    "N_STEPS = int(4.0e3)\n",
    "N_VAL = 500\n",
    "VAL_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f0c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = os.path.join(ROOT, 'figures/training/')\n",
    "create_dir(FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b31810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08708c35",
   "metadata": {},
   "source": [
    "# Data & network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004b89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / N_RES  # alpha scale for the residual connection\n",
    "\n",
    "# f^* parameters\n",
    "w_star = torch.rand(size=(INPUT_DIM, 1), requires_grad=False)\n",
    "y_star = torch.randn(size=(1,), requires_grad=False)\n",
    "\n",
    "# train data\n",
    "X_train = torch.randn(size=(N_STEPS, BATCH_SIZE, INPUT_DIM), requires_grad=False)\n",
    "y_train = 2 * torch.tanh(y_star + torch.matmul(X_train, w_star))**2 \n",
    "\n",
    "# val data\n",
    "x_val = torch.randn(size=(N_VAL, INPUT_DIM), requires_grad=False)\n",
    "y_val = 2 * torch.tanh(y_star + torch.matmul(x_val, w_star))**2\n",
    "\n",
    "# network & optimizer\n",
    "net = MuPResNet(input_dim=INPUT_DIM, width=WIDTH, activation=ACTIVATION, bias=BIAS, alpha=alpha, n_res=N_RES)\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=BASE_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eae8a1",
   "metadata": {},
   "source": [
    "### Train nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52667367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [43:07<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 4,000 iterations with L=500, m=256 : 43.13 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ALPHA = 1/L\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "s = time()\n",
    "val_iters = []\n",
    "ratios_train = []\n",
    "direct_ratios_train = []\n",
    "\n",
    "s = time()\n",
    "for t in tqdm(range(N_STEPS)):\n",
    "    x = X_train[t, : , :]\n",
    "    y = y_train[t, :, :]\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_hat = net(x)\n",
    "    loss = 0.5 * torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # gradient computation\n",
    "    opt.step()  # parameter update\n",
    "    train_losses.append(loss.detach().item())\n",
    "    \n",
    "    \n",
    "    if t % VAL_ITER == 0:\n",
    "        val_iters.append(t+1)\n",
    "        with torch.no_grad():\n",
    "            # compute and store ratios\n",
    "            h_0 = np.sqrt(net.d_model) * net.input_layer(x_val)  # muP scaling for the first layer\n",
    "            h_L = net.residual_layers(h_0)\n",
    "            h_0_norm = torch.norm(h_0, dim=1, p=2)\n",
    "            \n",
    "            ratios_train.append(torch.mean((torch.norm(h_L - h_0, dim=1,p=2)) /\n",
    "                                           h_0_norm).detach().item())\n",
    "            direct_ratios_train.append(torch.mean((torch.norm(h_L, dim=1,p=2)) /\n",
    "                                                  h_0_norm).detach().item())\n",
    "            \n",
    "            # compute and store loss\n",
    "            y_hat_val = net.output_layer(h_L) / np.sqrt(net.d_model)\n",
    "            \n",
    "            #y_hat_val = net(x_val)\n",
    "            val_loss = 0.5 * torch.mean((y_hat_val - y_val) ** 2).detach().item()\n",
    "            val_losses.append(val_loss)\n",
    "        #print('Train loss at step {:,}: {:.5f}'.format(t, loss.detach().item()))\n",
    "        #print('Val loss at step {:,}: {:.5f}'.format(t, val_loss))\n",
    "\n",
    "e = time()\n",
    "print('Time for {:,} iterations with L={:,}, m={:,} : {:.2f} minutes'.format(N_STEPS, N_RES, WIDTH, (e - s)/60))\n",
    "\n",
    "train_losses = np.array(train_losses)\n",
    "val_losses = np.array(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe426bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████▋                                                             | 1790/4000 [42:47<23:06,  1.59it/s]"
     ]
    }
   ],
   "source": [
    "# ALPHA = 1/\\sqrt{L}\n",
    "alpha = 1 / np.sqrt(N_RES)\n",
    "\n",
    "# network & optimizer\n",
    "net_g = MuPResNet(input_dim=INPUT_DIM, width=WIDTH, activation=ACTIVATION, bias=BIAS, alpha=alpha, n_res=N_RES)\n",
    "opt_g = torch.optim.SGD(params=net_g.parameters(), lr=BASE_LR)\n",
    "\n",
    "train_losses_g = []\n",
    "val_losses_g = []\n",
    "s = time()\n",
    "val_iters_g = []\n",
    "#ratios_train = []\n",
    "#direct_ratios_train = []\n",
    "\n",
    "s = time()\n",
    "for t in tqdm(range(N_STEPS)):\n",
    "    x = X_train[t, : , :]\n",
    "    y = y_train[t, :, :]\n",
    "    opt_g.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_hat = net_g(x)\n",
    "    loss = 0.5 * torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # gradient computation\n",
    "    opt_g.step()  # parameter update\n",
    "    train_losses_g.append(loss.detach().item())\n",
    "    \n",
    "    \n",
    "    if t % VAL_ITER == 0:\n",
    "        val_iters.append(t+1)\n",
    "        with torch.no_grad():\n",
    "            # compute and store loss\n",
    "            y_hat_val = net_g(x_val)\n",
    "            \n",
    "            #y_hat_val = net(x_val)\n",
    "            val_loss = 0.5 * torch.mean((y_hat_val - y_val) ** 2).detach().item()\n",
    "            val_losses_g.append(val_loss)\n",
    "        #print('Train loss at step {:,}: {:.5f}'.format(t, loss.detach().item()))\n",
    "        #print('Val loss at step {:,}: {:.5f}'.format(t, val_loss))\n",
    "\n",
    "e = time()\n",
    "print('Time for {:,} iterations with L={:,}, m={:,} : {:.2f} minutes'.format(N_STEPS, N_RES, WIDTH, (e - s)/60))\n",
    "\n",
    "train_losses_g = np.array(train_losses_g)\n",
    "val_losses_g = np.array(val_losses_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ad8b2",
   "metadata": {},
   "source": [
    "### Train pure linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.nn.Linear(in_features=INPUT_DIM, out_features=1, bias=BIAS)\n",
    "with torch.no_grad():\n",
    "    w.weight.data.copy_(torch.zeros_like(w.weight.detach().data))\n",
    "opt_w = torch.optim.SGD(params=w.parameters(), lr=0.05)\n",
    "lin_losses = []\n",
    "lin_val_losses = []\n",
    "lin_val_iters = []\n",
    "for t in tqdm(range(2500)):\n",
    "    x = X_train[t%N_STEPS, : , :]\n",
    "    y = y_train[t%N_STEPS, :, :]\n",
    "    opt_w.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_hat = w(x)\n",
    "    loss = 0.5 * torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # gradient computation\n",
    "    opt_w.step()  # parameter update\n",
    "    lin_losses.append(loss.detach().item())\n",
    "    \n",
    "    \n",
    "    if t % VAL_ITER == 0:\n",
    "        lin_val_iters.append(t+1)\n",
    "        with torch.no_grad():\n",
    "            y_hat_val = w(x_val)\n",
    "            \n",
    "            #y_hat_val = net(x_val)\n",
    "            val_loss = 0.5 * torch.mean((y_hat_val - y_val) ** 2).detach().item()\n",
    "            lin_val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e9cd1",
   "metadata": {},
   "source": [
    "### Train linear over first layer features (h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_feat = torch.nn.Linear(in_features=net.d_model, out_features=1, bias=BIAS)\n",
    "#with torch.no_grad():\n",
    "#    w_feat.weight.data.copy_(torch.zeros_like(w_feat.weight.detach().data))\n",
    "opt_w_feat = torch.optim.SGD(params=w_feat.parameters(), lr=0.05)\n",
    "lin_feat_losses = []\n",
    "lin_feat_val_losses = []\n",
    "lin_feat_val_iters = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    h0_val = np.sqrt(net.d_model) * net.input_layer(x_val)\n",
    "\n",
    "for t in tqdm(range(5000)):\n",
    "    x = X_train[t%N_STEPS, : , :]\n",
    "    y = y_train[t%N_STEPS, :, :]\n",
    "    with torch.no_grad():\n",
    "        h0 = np.sqrt(net.d_model) * net.input_layer(x)  # muP scaling for the first layer\n",
    "    opt_w_feat.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_hat = w_feat(h0) / net.d_model\n",
    "    loss = 0.5 * torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # gradient computation\n",
    "    opt_w_feat.step()  # parameter update\n",
    "    lin_feat_losses.append(loss.detach().item())\n",
    "    \n",
    "    \n",
    "    if t % VAL_ITER == 0:\n",
    "        lin_feat_val_iters.append(t+1)\n",
    "        with torch.no_grad():\n",
    "            y_hat_val = w_feat(h0_val) / net.d_model\n",
    "            \n",
    "            #y_hat_val = net(x_val)\n",
    "            val_loss = 0.5 * torch.mean((y_hat_val - y_val) ** 2).detach().item()\n",
    "            lin_feat_val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf0030",
   "metadata": {},
   "source": [
    "### Train linear over last layer features (h_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wL_feat = torch.nn.Linear(in_features=net.d_model, out_features=1, bias=BIAS)\n",
    "#with torch.no_grad():\n",
    "#    w_feat.weight.data.copy_(torch.zeros_like(w_feat.weight.detach().data))\n",
    "opt_wL_feat = torch.optim.SGD(params=wL_feat.parameters(), lr=0.2)\n",
    "linL_feat_losses = []\n",
    "linL_feat_val_losses = []\n",
    "linL_feat_val_iters = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    hLs = [net.residual_layers(np.sqrt(net.d_model) * net.input_layer(X_train[t, : , :]))\n",
    "           for t in tqdm(range(N_STEPS))]\n",
    "    hLs = torch.stack(hLs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    h0_val = np.sqrt(net.d_model) * net.input_layer(x_val)\n",
    "    hL_val = net.residual_layers(h0_val)\n",
    "\n",
    "for t in tqdm(range(8000)):\n",
    "    hL = hLs[t%N_STEPS, :, :]\n",
    "    opt_wL_feat.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_hat = wL_feat(hL) / net.d_model\n",
    "    loss = 0.5 * torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # gradient computation\n",
    "    opt_wL_feat.step()  # parameter update\n",
    "    linL_feat_losses.append(loss.detach().item())\n",
    "    \n",
    "    \n",
    "    if t % VAL_ITER == 0:\n",
    "        linL_feat_val_iters.append(t+1)\n",
    "        with torch.no_grad():\n",
    "            y_hat_val = wL_feat(hL_val) / net.d_model\n",
    "            \n",
    "            #y_hat_val = net(x_val)\n",
    "            val_loss = 0.5 * torch.mean((y_hat_val - y_val) ** 2).detach().item()\n",
    "            linL_feat_val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb69b20",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647356b1",
   "metadata": {},
   "source": [
    "## 1. Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = 'o'\n",
    "#marker = 'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(lin_feat_losses)), lin_feat_losses, label='train')\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "plt.plot(lin_feat_val_iters, lin_feat_val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "plt.ylabel('Lin-0-Features Loss', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = 'o'\n",
    "#marker = 'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(linL_feat_losses)), linL_feat_losses, label='train')\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "plt.plot(linL_feat_val_iters, linL_feat_val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "plt.ylabel('Lin-L-Features Loss', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4166d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = 'o'\n",
    "#marker = 'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(lin_losses)), lin_losses, label='train')\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "plt.plot(lin_val_iters, lin_val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "plt.ylabel('Lin Loss', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0453a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = 'o'\n",
    "#marker = 'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train')\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "plt.plot(val_iters, val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "plt.ylabel('ResNet Loss', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = 'o'\n",
    "#marker = 'o'\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(train_losses_g)), train_losses_g, label='train')\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "#plt.plot(val_iters, val_losses_g, label='val', marker=marker)\n",
    "plt.plot(np.arange(start=0, stop=len(train_losses_g), step=VAL_ITER), val_losses_g, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "plt.ylabel('ResNet-G Loss', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad4ab8",
   "metadata": {},
   "source": [
    "## 2. Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(ratios_train)), ratios_train)\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "#plt.plot(val_iters, val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "#plt.ylabel('$||h^L(t) - h^0(t)|| \\, / ||h^0(t)||$', fontsize=28)\n",
    "plt.ylabel('$\\\\frac{||h^L(t) - h^0(t)||}{||h^0(t)||}$', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "#plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0eab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#T_MAX = 100\n",
    "\n",
    "#plt.plot(np.arange(N_STEPS), train_losses, label='train')\n",
    "plt.plot(np.arange(len(ratios_train)), direct_ratios_train)\n",
    "#plt.plot(np.arange(start=0, stop=N_STEPS, step=VAL_ITER), val_losses, label='val')\n",
    "#plt.plot(val_iters, val_losses, label='val', marker=marker)\n",
    "\n",
    "plt.xlabel('Iteration $t$', fontsize=28)\n",
    "#plt.ylabel('$||h^L(t) - h^0(t)|| \\, / ||h^0(t)||$', fontsize=28)\n",
    "plt.ylabel('$||h^L(t)|| \\, / \\, ||h^0(t)||}$', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "#plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834e9dc",
   "metadata": {},
   "source": [
    "## Norms ratio after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = dict()\n",
    "direct_ratios = dict()\n",
    "#grad_ratios = dict()\n",
    "#direct_grad_ratios = dict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    h = np.sqrt(net.d_model) * net.input_layer(x)  # muP scaling for the first layer\n",
    "    h_0 = deepcopy(h.detach())\n",
    "    h_0_norm = torch.norm(h, dim=1, p=2)\n",
    "    h_L = net.residual_layers(h)\n",
    "    #h_0.retain_grad()\n",
    "    for l in range(N_RES):\n",
    "        layer = net.residual_layers[l]\n",
    "        h = layer(h)\n",
    "        ratios[l+1] = torch.mean(torch.norm(h - h_0, dim=1, p=2) / h_0_norm).item()\n",
    "        direct_ratios = torch.mean(torch.norm(h, dim=1, p=2) / h_0_norm).item()\n",
    "    \n",
    "    torch.testing.assert_close(h, h_L, rtol=1e-4, atol=1e-4)\n",
    "    #h_L.retain_grad()\n",
    "\n",
    "    #y_hat = net.output_layer(h_L) / np.sqrt(net.d_model)  # muP scaling for the last layer\n",
    "    #loss = 0.5 * torch.mean((y_hat - y)**2)\n",
    "    #loss.backward()  # gradient computation\n",
    "\n",
    "    # forward norms\n",
    "\n",
    "    # backward (grads) norms\n",
    "    #dh_L_norm = torch.norm(h_L.grad, dim=1, p=2)\n",
    "    #grad_ratios_[n_res].append(torch.mean((torch.norm(h_0.grad - h_L.grad, dim=1,p=2)) /\n",
    "    #                           dh_L_norm).detach().item())\n",
    "    #direct_grad_ratios_[n_res].append(torch.mean((torch.norm(h_0.grad, dim=1,p=2)) / \n",
    "    #                                             dh_L_norm).detach().item())\n",
    "\n",
    "    #grad_ratios[beta] = grad_ratios_\n",
    "    #direct_grad_ratios[beta] = direct_grad_ratios_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14485e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log')\n",
    "marker = None\n",
    "#plt.xscale('log')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "plt.plot(np.arange(1, N_RES+1), [ratios[l+1] for l in range(N_RES)], marker=marker)\n",
    "plt.plot(np.arange(1, N_RES+1), [np.sqrt((1 + alpha**2)**l - 1) for l in range(1, N_RES+1)], linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Layer $l$', fontsize=28)\n",
    "plt.ylabel('$||h^l - h^0|| \\, / ||h^0||$', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "#plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b110d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.yscale('log')\n",
    "marker = None\n",
    "#plt.xscale('log')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "plt.plot(np.arange(1, N_RES+1), [ratios[l+1] for l in range(N_RES)], marker=marker)\n",
    "plt.plot(np.arange(1, N_RES+1), [np.sqrt((1 + alpha**2)**l - 1) for l in range(1, N_RES+1)], linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Layer $l$', fontsize=28)\n",
    "plt.ylabel('$||h^l - h^0|| \\, / ||h^0||$', fontsize=28)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "#plt.legend(fontsize=18)\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'ratio.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967c5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
